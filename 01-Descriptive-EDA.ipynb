{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bda30aa-3910-449b-a740-401813796607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import nlt\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd15d796-83d4-44a1-b3cb-ebae77a0fb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# --- DATA LOADING ---\n",
    "df = pd.DataFrame() # Initialize df outside the try block to prevent NameError\n",
    "try:\n",
    "    # ðŸŽ¯ CONFIRMED ABSOLUTE PATH: This bypasses all relative path issues.\n",
    "    absolute_path = r'C:\\Users\\deres\\OneDrive\\Desktop\\week1\\week1\\data\\raw_analyst_ratings.csv'\n",
    "    \n",
    "    df = pd.read_csv(absolute_path)\n",
    "    print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"FATAL ERROR: Could not load data. Details: {e}\")\n",
    "    # df remains an empty DataFrame if loading fails\n",
    "\n",
    "# --- Proceed ONLY if df was created successfully ---\n",
    "if not df.empty:\n",
    "    # Convert the 'date' column to datetime objects\n",
    "    # FIX: Use explicit format to solve the ValueError:\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "\n",
    "    # Display first few rows to confirm loading and structure\n",
    "    print(\"\\n--- Initial Data Head ---\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"\\nAborting further analysis: DataFrame (df) is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99d32ec-79a5-48f5-b4fb-2120d5914b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# --- DATA LOADING ---\n",
    "try:\n",
    "    # ðŸš¨ UPDATE THIS LINE with the path found in Cell 1 (e.g., '../data/raw_analyst_ratings.csv')\n",
    "    df = pd.read_csv('raw_analyst_ratings.csv')\n",
    "    print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"FATAL ERROR: Could not load data. Details: {e}\")\n",
    "    # We stop here if the DataFrame is not created\n",
    "    \n",
    "# --- Proceed ONLY if df was created successfully ---\n",
    "if not df.empty:\n",
    "    # Convert the 'date' column to datetime objects\n",
    "    # FIX: Re-add the specific format to avoid the ValueError that causes NaT\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "\n",
    "    # Display first few rows to confirm loading and structure\n",
    "    print(\"\\n--- Initial Data Head ---\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"\\nAborting further analysis: DataFrame (df) is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d542c852-067c-4758-b3f4-2759d057e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate Lengths\n",
    "if not df.empty:\n",
    "    df['headline_length'] = df['headline'].apply(len)\n",
    "    df['headline_word_count'] = df['headline'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "    # 2. Print Basic Statistics\n",
    "    print(\"\\n--- Headline Length Statistics (Characters) ---\")\n",
    "    print(df['headline_length'].describe())\n",
    "\n",
    "    # 3. Visualize Distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['headline_length'], bins=50, kde=True, color='purple')\n",
    "    plt.title('Distribution of Headline Lengths (Characters)')\n",
    "    plt.xlabel('Character Length')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1b3d92-61bb-4492-a392-a0ef15963c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # 1. Count Articles per Publisher\n",
    "    publisher_counts = df['publisher'].value_counts()\n",
    "\n",
    "    print(\"\\n--- Top 10 Most Active Publishers ---\")\n",
    "    print(publisher_counts.head(10))\n",
    "\n",
    "    # 2. Visualize Top Publishers\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    publisher_df = publisher_counts.head(10).reset_index()\n",
    "    publisher_df.columns = ['Publisher', 'Count']\n",
    "    sns.barplot(x='Count', y='Publisher', data=publisher_df, palette='viridis')\n",
    "    plt.title('Top 10 Publishers by Article Count')\n",
    "    plt.xlabel('Number of Articles')\n",
    "    plt.ylabel('Publisher')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec6a117-d0f5-4d21-b341-3b38b103acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # 1. Analyze Trends by Day of the Week\n",
    "    df['day_of_week'] = df['date'].dt.day_name()\n",
    "    weekly_counts = df['day_of_week'].value_counts().reindex([\n",
    "        'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'\n",
    "    ])\n",
    "\n",
    "    print(\"\\n--- Article Counts by Day of the Week ---\")\n",
    "    print(weekly_counts)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    weekly_counts.plot(kind='bar', color='darkorange')\n",
    "    plt.title('Article Frequency by Day of the Week')\n",
    "    plt.ylabel('Article Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Analyze Trends Over Time (Monthly)\n",
    "    df['publication_month'] = df['date'].dt.to_period('M')\n",
    "    monthly_counts = df['publication_month'].astype(str).value_counts().sort_index()\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    monthly_counts.plot(kind='line', marker='o', color='teal')\n",
    "    plt.title('Article Frequency Over Time (Monthly)')\n",
    "    plt.xlabel('Date (Month)')\n",
    "    plt.ylabel('Article Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc4338-6e15-4752-aa2c-02508e864695",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "# Download necessary NLTK data (run this once)\n",
    "try:\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('stopwords')\n",
    "except:\n",
    "    pass # Already downloaded\n",
    "\n",
    "# Initialize Lemmatizer and define custom stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Add common finance/news terms to the standard list of stop words\n",
    "CUSTOM_STOPWORDS = set(stopwords.words('english') + [\n",
    "    'stock', 'price', 'target', 'rating', 'analyst', 'maintains', \n",
    "    'upgrade', 'downgrade', 'buy', 'sell', 'hold', 'news', 'market', \n",
    "    'week', 'session', 'hit', 'high', 'low'\n",
    "])\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Clean, tokenize, remove stopwords, and lemmatize text.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "\n",
    "    # 1. Tokenize and remove short tokens\n",
    "    tokens = [token for token in simple_preprocess(str(text), deacc=True) if len(token) >= 3]\n",
    "    \n",
    "    # 2. Remove stop words\n",
    "    tokens = [token for token in tokens if token not in CUSTOM_STOPWORDS]\n",
    "    \n",
    "    # 3. Lemmatization (reducing words to their base form)\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to the 'headline' column\n",
    "if not df.empty:\n",
    "    df['processed_headline'] = df['headline'].apply(preprocess)\n",
    "    print(\"Preprocessing complete. Sample:\")\n",
    "    print(df[['headline', 'processed_headline']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec05c463-c5ea-4cc5-8eb8-5d964e84766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "\n",
    "if not df.empty:\n",
    "    # 1. Create a Dictionary and Corpus for LDA\n",
    "    # Dictionary maps each unique word to an ID\n",
    "    dictionary = Dictionary(df['processed_headline'].tolist())\n",
    "    \n",
    "    # Filter out tokens that appear in less than 15 documents or more than 50% of the documents\n",
    "    dictionary.filter_extremes(no_below=15, no_above=0.5)\n",
    "    \n",
    "    # Corpus is a list of (token_id, token_count) tuples for each document\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in df['processed_headline'].tolist()]\n",
    "    \n",
    "    # 2. Train the LDA Model\n",
    "    NUM_TOPICS = 5 # A good starting point, can be optimized later\n",
    "\n",
    "    lda_model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=NUM_TOPICS,\n",
    "        random_state=42,\n",
    "        chunksize=100,\n",
    "        passes=10,\n",
    "        per_word_topics=True\n",
    "    )\n",
    "\n",
    "    # 3. Print the Topics and Keywords\n",
    "    print(f\"\\n--- LDA Model Topics (N={NUM_TOPICS}) ---\")\n",
    "    for idx, topic in lda_model.print_topics(num_words=5):\n",
    "        print(f\"Topic {idx}: {topic}\")\n",
    "    \n",
    "    # 4. Assign the dominant topic back to the DataFrame\n",
    "    def format_topics_sentences(ldamodel, corpus, texts):\n",
    "        sent_topics_df = pd.DataFrame()\n",
    "        \n",
    "        # Get dominant topic for each document\n",
    "        for i, row_list in enumerate(ldamodel[corpus]):\n",
    "            row = row_list[0] if lda_model.per_word_topics else row_list\n",
    "            # Ensure row is not empty\n",
    "            if not row:\n",
    "                continue\n",
    "\n",
    "            row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "            # Get the Dominant Topic and its probability (Perc_Contribution)\n",
    "            dominant_topic = row[0][0]\n",
    "            perc_contribution = row[0][1]\n",
    "            \n",
    "            # Get the keywords for that topic\n",
    "            topic_keywords = ldamodel.print_topic(dominant_topic, 5)\n",
    "            \n",
    "            sent_topics_df = pd.concat([\n",
    "                sent_topics_df,\n",
    "                pd.Series([int(dominant_topic), round(perc_contribution, 4), topic_keywords]).to_frame().T\n",
    "            ], ignore_index=True)\n",
    "            \n",
    "        sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "        \n",
    "        # Add the original headline\n",
    "        contents = pd.Series(texts)\n",
    "        sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "        return sent_topics_df\n",
    "\n",
    "    df_topic_keywords = format_topics_sentences(lda_model, corpus, df['headline'].tolist())\n",
    "    \n",
    "    # Show how the topics map to the headlines\n",
    "    print(\"\\n--- Headlines with Dominant Topic Assignment ---\")\n",
    "    print(df_topic_keywords.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa45162-83ef-4836-a8eb-ac3d24422cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is loaded and 'date' column is datetime\n",
    "if not df.empty:\n",
    "    print(\"--- Time Series Analysis: Publication Frequency ---\")\n",
    "    \n",
    "    # 1. Monthly Frequency Trend\n",
    "    df['publication_month'] = df['date'].dt.to_period('M')\n",
    "    monthly_counts = df['publication_month'].value_counts().sort_index()\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    monthly_counts.plot(kind='line', marker='o', color='teal', linewidth=2)\n",
    "    plt.title('Article Frequency Over Time (Monthly)')\n",
    "    plt.xlabel('Date (Month)')\n",
    "    plt.ylabel('Article Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Daily Frequency Trend (Can help spot specific event spikes)\n",
    "    df['publication_day'] = df['date'].dt.to_period('D')\n",
    "    daily_counts = df['publication_day'].value_counts().sort_index()\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    daily_counts.plot(kind='line', color='darkred', linewidth=1)\n",
    "    plt.title('Article Frequency Over Time (Daily Spikes)')\n",
    "    plt.xlabel('Date (Day)')\n",
    "    plt.ylabel('Article Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e92d59-73df-4cd5-a452-dac9dcec5eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    print(\"--- Intraday Timing Analysis: Peak Hours ---\")\n",
    "    \n",
    "    # Extract the hour of publication\n",
    "    df['publication_hour'] = df['date'].dt.hour\n",
    "    \n",
    "    # Count the frequency by hour\n",
    "    hourly_counts = df['publication_hour'].value_counts().sort_index()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Use a bar plot for clear hourly comparison\n",
    "    sns.barplot(x=hourly_counts.index, y=hourly_counts.values, palette='viridis')\n",
    "    \n",
    "    plt.title('News Publication Frequency by Hour of the Day')\n",
    "    plt.xlabel('Hour of Day (24h Clock)')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.xticks(range(0, 24)) # Ensure all 24 hours are represented on the x-axis\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify and print the top 3 peak hours\n",
    "    top_hours = hourly_counts.nlargest(3)\n",
    "    print(\"\\nTop 3 Peak News Publication Hours:\")\n",
    "    for hour, count in top_hours.items():\n",
    "        print(f\"Hour {hour}:00 (e.g., {hour}:00 AM/PM) with {count:,} articles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010ea0ea-9dd4-4de4-a908-29c6b5573310",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not df.empty:\n",
    "    print(\"--- 1. Publisher Dominance Analysis ---\")\n",
    "    \n",
    "    # 1.1 Analyze Raw Publisher Counts (Recap from EDA)\n",
    "    raw_publisher_counts = df['publisher'].value_counts()\n",
    "    print(\"Top 10 Raw Publisher Names (by volume):\")\n",
    "    print(raw_publisher_counts.head(10))\n",
    "    \n",
    "    # -----------------------------------------------------\n",
    "    \n",
    "    print(\"\\n--- 2. Domain Normalization (Identifying Organizations) ---\")\n",
    "    \n",
    "    # Function to extract the domain name from a publisher string\n",
    "    def extract_domain(publisher):\n",
    "        publisher = str(publisher).lower().strip()\n",
    "        # If it contains an '@' sign (likely an email), extract the domain part\n",
    "        if '@' in publisher:\n",
    "            return publisher.split('@')[-1]\n",
    "        # For non-email names, return the name itself (or standardize it)\n",
    "        # We can also attempt to extract domains from URLs if available, but for now, we focus on normalizing names.\n",
    "        return publisher\n",
    "\n",
    "    # Apply domain extraction\n",
    "    df['publisher_domain'] = df['publisher'].apply(extract_domain)\n",
    "    \n",
    "    # Analyze the normalized domain counts\n",
    "    domain_counts = df['publisher_domain'].value_counts()\n",
    "    print(\"Top 10 Normalized Publishers (by domain/volume):\")\n",
    "    print(domain_counts.head(10))\n",
    "    \n",
    "    # -----------------------------------------------------\n",
    "    \n",
    "    print(\"\\n--- 3. Topic Difference by Dominant Publisher ---\")\n",
    "    \n",
    "    # Select the top 3-5 dominant domains for topic analysis\n",
    "    top_domains = domain_counts.head(5).index.tolist()\n",
    "    \n",
    "    # Assuming you have run the LDA Topic Modeling and have an 'processed_headline' column\n",
    "    # If you haven't run the LDA yet, skip this section or run Cells 4 & 5 first.\n",
    "    \n",
    "    if 'processed_headline' in df.columns:\n",
    "        # Re-run the LDA assignment for the full dataset (from your previous LDA cell)\n",
    "        # Note: This is computationally heavy; only run if Topic Modeling was successful.\n",
    "        \n",
    "        # --- (Required setup from LDA Cell 5: Dictionary, Corpus, and lda_model) ---\n",
    "        # Since I don't have the lda_model object here, I will structure the analysis\n",
    "        # to focus on keywords as a proxy for topic differences.\n",
    "        \n",
    "        print(f\"Analyzing headline keywords for the top 5 domains: {top_domains}\")\n",
    "        \n",
    "        for domain in top_domains:\n",
    "            # Filter the DataFrame for the current domain\n",
    "            domain_df = df[df['publisher_domain'] == domain]\n",
    "            \n",
    "            # Combine all processed headlines into one large string\n",
    "            all_text = ' '.join([' '.join(tokens) for tokens in domain_df['processed_headline']])\n",
    "            \n",
    "            # Count the frequency of words\n",
    "            word_counts = pd.Series(all_text.split()).value_counts()\n",
    "            \n",
    "            print(f\"\\n--- Top 5 Keywords for {domain} ({len(domain_df)} articles) ---\")\n",
    "            print(word_counts.head(5).to_string())\n",
    "            \n",
    "        print(\"\\nInterpretation:\")\n",
    "        print(\"By comparing the top keywords across domains, you can infer differences in their reporting focus (e.g., one focuses on 'merger' and 'acquisition', another on 'economic' and 'report').\")\n",
    "    else:\n",
    "        print(\"\\nTopic analysis skipped: 'processed_headline' column not found. Please run Topic Modeling first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
